{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from monkeylearn import MonkeyLearn\n",
    "from sklearn.linear_model import LogisticRegression # importing Sklearn's logistic regression's module\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing # preprossing is what we do with the data before we run the learning algorithm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/NishantDate/ML_final_project/main/train.tsv'\n",
    "test_url = 'https://raw.githubusercontent.com/NishantDate/ML_final_project/main/test.tsv'\n",
    "valid_url = 'https://raw.githubusercontent.com/NishantDate/ML_final_project/main/valid.tsv'\n",
    "names = ['ID', 'label', 'statement', 'subject(s)', 'speaker', 'speaker title', 'state', 'party', 'barely true', 'false', 'half-true', 'mostly-true', 'pants on fire', 'context']\n",
    "\n",
    "train = pd.read_csv(train_url, sep='\\t', names = names )\n",
    "test = pd.read_csv(test_url, sep='\\t', names = names)\n",
    "valid = pd.read_csv(valid_url, sep='\\t',names = names)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "#encoding labels\n",
    "true_dict = {'pants-fire':0, 'false':0,'barely-true':0, 'half-true':1, 'mostly-true':1, 'true':1}\n",
    "\n",
    "train['label'] = train['label'].apply(lambda x: true_dict[x] if x in true_dict.keys() else np.nan)\n",
    "valid['label'] = valid['label'].apply(lambda x: true_dict[x] if x in true_dict.keys() else np.nan)\n",
    "test['label'] = test['label'].apply(lambda x: true_dict[x] if x in true_dict.keys() else np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def generate_truth(barely_true, false, half_true, mostly_true, pants_on_fire):\n",
    "  #generates the probability of the speaker telling the truth\n",
    "  arr = np.array([barely_true, false, half_true, mostly_true, pants_on_fire])\n",
    "  truths = barely_true + half_true + mostly_true\n",
    "  total = sum(arr)\n",
    "  score = truths / total #probability that the speaker will tell the truth\n",
    "  return score\n",
    "\n",
    "train['truth_score'] = train.apply(lambda x: generate_truth(x['barely true'],x['false'], x['half-true'], x['mostly-true'], x['pants on fire']), axis = 1)\n",
    "#train[['truth_score']] = scaler.fit_transform(train[['truth_score']])\n",
    "test['truth_score'] = test.apply(lambda x: generate_truth(x['barely true'],x['false'], x['half-true'], x['mostly-true'], x['pants on fire']), axis = 1)\n",
    "#test[['truth_score']] = scaler.fit_transform(test[['truth_score']])\n",
    "valid['truth_score'] = valid.apply(lambda x: generate_truth(x['barely true'],x['false'], x['half-true'], x['mostly-true'], x['pants on fire']), axis = 1)\n",
    "#valid[['truth_score']] = scaler.fit_transform(valid[['truth_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding parties -- republican: 0 , democrat: 1, Other:2\n",
    "\n",
    "true_dict_party = {'republican':0, 'democrat': 1, 'other': 2 }\n",
    "train['party'] = train['party'].apply(lambda x: true_dict_party[x] if x in true_dict_party.keys() else true_dict_party['other'])\n",
    "valid['party'] = valid['party'].apply(lambda x: true_dict_party[x] if x in true_dict_party.keys() else true_dict_party['other'])\n",
    "test['party'] = test['party'].apply(lambda x: true_dict_party[x] if x in true_dict_party.keys() else true_dict_party['other'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-orlando",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number = LabelEncoder()\n",
    "train['state'] = number.fit_transform(train['state'].astype('str'))\n",
    "test['state'] = number.fit_transform(test['state'].astype('str'))\n",
    "valid['state'] = number.fit_transform(valid['state'].astype('str'))                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna()\n",
    "test.dropna()\n",
    "valid.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-couple",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Do this with test train split \n",
    "\n",
    "#Normalize -- the occurences \n",
    "#Sentiment analysis on Statement\n",
    "#Bag of words\n",
    "\n",
    "X_train = train[['party', 'state', 'truth_score']]\n",
    "Y_train = train['label']\n",
    "X_test = test[['party', 'state', 'truth_score']]\n",
    "Y_test = test['label']\n",
    "\n",
    "X_train = X_train.fillna(value = X_train.mean())\n",
    "Y_train = Y_train.fillna(value = Y_train.mean())\n",
    "X_test = X_test.fillna(value = X_test.mean())\n",
    "Y_test = Y_test.fillna(value = Y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-grant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store values of test set accuracy.\n",
    "acc_test = []\n",
    "acc_train_logreg = []\n",
    "acc_test_logreg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-manhattan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store different values of parameter 'c'.\n",
    "c_logreg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-fetish",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_test = []\n",
    "\n",
    "def base_line_median(Y_train):\n",
    "    #print(\"BASELINE FUNCTION\")\n",
    "    return np.median(Y_train)\n",
    "def base_line_mean(Y_train):\n",
    "    return np.mean(Y_train)\n",
    "def base_line_median_accuracy(X_train, Y_train, X_test, Y_test):\n",
    "    Y_hat_train = np.array([])\n",
    "    for Y in Y_train:\n",
    "        Y_hat_train = np.append(Y_hat_train,base_line_median(Y_train))\n",
    "        \n",
    "    baseline_train = np.mean(Y_hat_train == Y_train)\n",
    "    baseline_test.append(acc_test)\n",
    "    print(\"Accuracy on test data = %f\" % baseline_train)\n",
    "    \n",
    "def base_line_mean_accuracy(X_train, Y_train, X_test, Y_test):\n",
    "    Y_hat_train = np.array([])\n",
    "    for Y in Y_train:\n",
    "        Y_hat_train = np.append(Y_hat_train,base_line_mean(Y_train))\n",
    "        \n",
    "    baseline_train = np.mean(Y_hat_train == Y_train)\n",
    "    baseline_test.append(baseline_train)\n",
    "    print(\"Accuracy on test data = %f\" % baseline_train)\n",
    "\n",
    "base_line_median_accuracy(X_train, Y_train, X_test, Y_test)\n",
    "base_line_mean_accuracy(X_train, Y_train, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-adventure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the function logreg that accepts 'c' as a parameter, which is used to create logreg model \n",
    "# with different values of C.\n",
    "from sklearn import linear_model\n",
    "def logreg_model(c , X_train, Y_train, X_test, Y_test):\n",
    "    # Create an object of logistic regression model using linear_model.\n",
    "    # Pass the value of penalty as 'L1'. By default, it is 'L2'.\n",
    "    # Pass the value of C = c. Note that C is the inverse of lambda. So, small value of C i.e. b/w 0 and 1 \n",
    "    # means stronger regularization and large value means less regularization.\n",
    "    # Also, in sklearn, L1 is only supported with solver = 'saga'. Solver is the type of optimization algorithm like GDA or\n",
    "    # SGDA, which is to be used. So, 'saga' is another algorithm like that. Pass the value of solver as 'saga'\n",
    "\n",
    "    # TODO - Create the Logistic Regression model object as described above and save it to logreg - 5 points\n",
    "    logreg = linear_model.LogisticRegression(C=c,penalty='l1', warm_start=True, solver='saga')\n",
    "    \n",
    "    # TODO - Fit the model on the training set - 5 points\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    \n",
    "    # TODO - Find the prediction on training set - 5 points\n",
    "    Yhat_train = logreg.predict(X_train)\n",
    "    \n",
    "    # Adding training accuracy to acc_train_logreg\n",
    "    acc_train = np.mean(Yhat_train == Y_train)\n",
    "    acc_train_logreg.append(acc_train)\n",
    "    print(\"Accuracy on training data = %f\" % acc_train)\n",
    "    \n",
    "    # TODO - Find the prediction on test set - 5 points\n",
    "    Yhat_test = logreg.predict(X_test)\n",
    "    \n",
    "    # Adding testing accuracy to acc_test_logreg\n",
    "    acc_test = np.mean(Yhat_test == Y_test)\n",
    "    acc_test_logreg.append(acc_test)\n",
    "    print(\"Accuracy on test data = %f\" % acc_test)\n",
    "    \n",
    "    # Appending value of c for graphing purposes\n",
    "    c_logreg.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "for c in cVals:\n",
    "    logreg_model(c, X_train.fillna(value = X_train.mean()), Y_train.fillna(value = Y_train.mean()), \n",
    "                 X_test.fillna(value = X_test.mean()), Y_test.fillna(value =Y_test.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_logreg = []\n",
    "acc_test_logreg = []\n",
    "c_logreg = []\n",
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "for c in cVals:\n",
    "    logreg_model(c, X_train, Y_train, X_test, Y_test)\n",
    "plt.plot(c_logreg, acc_train_logreg, 'ro-') \n",
    "plt.plot(c_logreg, acc_test_logreg,'bo-') \n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-conservation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-address",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store values of training set accuracy.\n",
    "acc_train_logreg2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-poultry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store values of test set accuracy.\n",
    "acc_test_logreg2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-geometry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store different values of parameter 'c'.\n",
    "c_logreg2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-gravity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logreg2_model(c , X_train, Y_train, X_test, Y_test):\n",
    "    # Create an object of logistic regression model using linear_model.\n",
    "    # Pass the value of C=c.\n",
    "    # You need not pass other parameters as penalty is 'L2' by default.\n",
    "    \n",
    "    # TODO - Create the Logistic Regression model object as described above and save it to logreg2 - 5 points\n",
    "    logreg2 = linear_model.LogisticRegression(C=c, warm_start=True)\n",
    "\n",
    "    \n",
    "    # TODO - Fit the model on the training set - 5 points\n",
    "\n",
    "    logreg2.fit(X_train, Y_train)\n",
    "\n",
    "    # TODO - Find the prediction on training set - 5 points\n",
    "    Yhat_train = logreg2.predict(X_train)\n",
    "    \n",
    "    # Adding training accuracy to acc_train_logreg2\n",
    "    \n",
    "    acc_train = np.mean(Yhat_train == Y_train)\n",
    "    acc_train_logreg2.append(acc_train)\n",
    "    print(\"Accuracy on training data = %f\" % acc_train)\n",
    "    \n",
    "    # TODO - Find the prediction on test set - 5 points\n",
    "    Yhat_test = logreg2.predict(X_test)\n",
    "    \n",
    "    # Adding testing accuracy to acc_test_logreg2\n",
    "    acc_test = np.mean(Yhat_test == Y_test)\n",
    "    acc_test_logreg2.append(acc_test)\n",
    "    print(\"Accuracy on test data = %f\" % acc_test)\n",
    "    \n",
    "    # Appending value of c for graphing purposes\n",
    "    c_logreg2.append(c)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the above function i.e. logreg_model with different values of parameter 'c'.\n",
    "# Start with smaller values of 'c' say 0.0001, 0.001, 0.01, 0.1, 1, 10, 100\n",
    "for c in cVals:\n",
    "    logreg2_model(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write code to plot 2 plots - 10 points\n",
    "# Plot training accuracy(Y-axis) v/s 'c' on X - Axis.\n",
    "# Plot test accuracy(Y-Axis) v/s 'c' on X - Axis.\n",
    "\n",
    "# IMP - Make sure you reinitialize c_logreg2, acc_train_logreg2 and acc_test_logreg2 before rerunning logreg2_model()\n",
    "acc_train_logreg2 = []\n",
    "acc_test_logreg2 = []\n",
    "c_logreg2 = []\n",
    "for c in cVals:\n",
    "    logreg2_model(c, X_train, Y_train, X_test, Y_test)\n",
    "plt.plot(c_logreg2, acc_train_logreg2, 'ro-') \n",
    "plt.plot(c_logreg2, acc_test_logreg2,'bo-') \n",
    "plt.grid()\n",
    "# Use the following function to have a legend\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z\n",
    "\n",
    "\n",
    "\n",
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    lmbda = 0.15\n",
    "    N = len(y)\n",
    "    row_len = X.shape[1]\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%100 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X.iloc[i,0:row_len], W, b)\n",
    "            #print(\"Feed forward DONE\")\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i]-a[l]))\n",
    "                    #print(\"lin alg done\")\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                        #print(\"hidden delta done\")\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                   # print(\"new tri done\")\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "       # print(\"W l done\")\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost \n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    row_len = X.shape[1]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X.iloc[i,0:row_len], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y\n",
    "\n",
    "nn_structure = [3, 4, 2]\n",
    "\n",
    "    \n",
    "# train the NN\n",
    "print(Y_train.shape)\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, Y_train, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-conjunction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-collective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-sentence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
